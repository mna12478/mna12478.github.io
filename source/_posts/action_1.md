---
title: 动作识别Large-scale Video Classification with Convolutional Neural Networks
date: 2019-09-25 16:27:58
categories: 深度学习
tags:
- 动作识别
- CNN
- Sports1M
---
&emsp;&emsp;动作识别，即判断视频中人的动作的类别，其难点包括类内和类间差距（同一个动作，不同的人的表现可能有极大的不同）、运动特征提取（很难确定动作的起始点）、缺少标注良好的大数据集等。以往的方法多是基于人工提取的特征，提取感兴趣区域的局部特征，并将不同的特征组合起来，得到一个固定长度的特征向量，最后对特征向量进行分类。在CNN能以较低的错误率为图像分类后，学者们也开始尝试将CNN用于动作的分类，本文就是将2DCNN用于动作识别的一次尝试。另外，还提出了一个大的动作识别的数据库Sports1M。
&emsp;&emsp;CNN识别图片的类别时，能将图片裁剪和缩放到固定尺寸，而与图片不同的是，视频在时间维度变化很大，不太容易处理成固定结构。本文探索了几种时域信息融合的方法/结构。
![](/images/Large/fuse.png "不同的时域信息融合方法")
# 一、时域信息融合网络
&emsp;&emsp;方法一，Single-frame(单帧)，使用一个单帧的基础结构（与图片识别相似）来了解静态外观对识别精度的贡献。与AlexNet类似，输入为170\*170\*3 (图片分类是224\*224\*3)，结构为C(96; 11; 3)-N-P-C(256; 5; 1)-N-P-C(384; 3; 1)-C(384; 3; 1)-C(256; 3; 1)-P-FC(4096)-FC(4096)，pooling层使用非叠加的2\*2区域，归一化层是LRN层，最后一层连接了一个softmax层。
&emsp;&emsp;方法二，Early Fusion(早期融合)，直接在pixel-level将整个时间窗的信息进行融合（将不同的帧沿通道维度合并）。更改第一个卷积层的filter，扩展成11\*11\*3\*T，T是时间跨度。早期融合并且pixel-level的直接融合使得网络能精准检测局部的运动方向和速度。
&emsp;&emsp;方法三，Late Fusion(晚期融合)，将两个分离的single-frame网络(距离为15，共享参数)，在第一个全连接层融合信息，因此，任何一个单独的single-frame网络都不能检测任何运动信息，但是第一个全连接层能通过比较两个网络的输出，来计算全局运动特征。
&emsp;&emsp;方法四，Slow Fusion(缓慢融合)，通过网络缓慢融合时间信息，从而高层能渐进得到更全局的时间和空间信息。通过扩展所有卷积层在时间维度的连接，并使用时间和空间维度的卷积来计算激活值，在我们使用的模型中，第一个卷积层扩充为，在10帧的输入clip中应用时间跨度T=4的filter，valid卷积，stride=2，在时间维度生成4个响应；第二、三层重复这个过程，时间跨度T=2，stride=2，因此，第三个卷积能获取整个输入的10个frame的信息。（直译过来就是，输入为10个frame，第一层卷积以4个frame为一组作为输入，使用类似于Early Fusion的卷积层，，组与组之间的间隔为2个frame；第二层以第一层的输出为输入，2个为一组，stride=2进行卷积；第三层以第二层的输出为输入，2个为一组）
# 二、网络结构优化
&emsp;&emsp;由于CNN的训练比较耗时，要减少耗时，可用的方法包括扩展硬件、权重量化、好的优化策略和初始化策略，本文采用的策略是，在不影响识别效果的前提下更改网络结构。如果使用减少层数和每层神经元个数的方法，那么会影响识别精度，所以本文采用的具体方法是多分辨率网络。原本的输入为178\*178，多分辨率网络分为两路：context stream和fovea stream，context stream的输入为原始输入经过降采样得到的89\*89的图片；fovea stream的输入是截取原始图像最中间的89\*89的区域，两个流的网络都是用的full frame的网络结构，不同的是最后的pooling层的尺寸是7\*7\*256（实际上，这里我怎么算也没能算出来7\*7）
![](/images/Large/multiresolution.png "多分辨率网络")
# 三、数据增广
&emsp;&emsp;方法：crop to center，resize to 200\*200，随机采样170\*170区域，以0.5的概率随机水平flip。同一个clip的所有帧处理方法一致，最后，每张图片像素值减去所有图片的均值117。
# 四、实验结果
## 1、Sports1M数据库
&emsp;&emsp;每一类有1000-3000个视频，5%的视频标注了不止一个标签，通过分析视频周围的文本元数据自动生成标注，所以数据是弱标注。70%-10%-20%的分组，由于YouTube上的视频是重复的，所以相同的视频可能既出现在训练集，也可能出现在测试集，通过近似重复查找算法，发现100万视频中有1755条在frame-level上是重复的。此外，由于只使用每个视频中最多100个半秒片段的随机组合，而且视频的平均长度是5分36秒，所以在数据分割中不太可能出现相同的帧
## 2、实验结果
&emsp;&emsp;训练：训练了一个月，全帧网络模型每秒钟处理5个clip，single model每秒处理20个clip。5个clip的速度比我们预想的慢了20倍，但是希望通过使用10-50个模型副本，总体上达到可比的速度
&emsp;&emsp;测试：Slow fusion效果最好。随机选择20个clip，每个clip预测4次（不同的crop和clip），将预测结果平均。为了生成video-level的预测，选择了在每个视频的持续时间内平均单个clip的预测，希望有更详细的技术可以进一步提高性能，但这些不在本文的讨论范围。测试集包括200000个视频，4000000个clip
![](/images/Large/results.png "实验结果")
&emsp;&emsp;以第一个卷积层为例，从结果来看，context流学习低频率、颜色特征；fovea流学习灰度、高频率的特征
![](/images/Large/filter.png "第一个卷积层学到的特征")
## 3、迁移学习
&emsp;&emsp;迁移至UCF-101上的结果：检验在Sports-1M上学到的特征是否是一般的特征。
### 1）UCF-101数据库
&emsp;&emsp;UCF-101包括13320个视频，101类，5个大组：人机交互（化眼妆、刷牙等）、身体运动（婴儿爬行、俯卧撑、吹蜡烛）、人人交互（头部按摩、理发）、运动
### 2）迁移学习
#### a、迁移学习方案
&emsp;&emsp;迁移学习：由于我们希望CNN能在网络底部学到更一般的特征（如边缘、局部形状），在顶端学到更复杂、数据集特有的特征，所以迁移学习的方案有：
&emsp;&emsp;a、fine-tune顶层：将CNN看做固定特征提取器，在最后的4096维层训练一个分类器，加上dropout，发现以10%的机会保持每个单元的积极性是有效的
&emsp;&emsp;b、fine-tune顶层的3层：除了重新训练最后的分类层外，考虑也重新训练两个FC层，在所有已训练层之前用dropout，以10%的机会保持单元积极性
&emsp;&emsp;c、fine-tune所有层：重新训练所有网络的参数，包括底部的卷积层
&emsp;&emsp;d、train fromscratch
![](/images/Large/transfer.png "不同迁移学习方案的结果")
&emsp;&emsp;结果：从每个video中采样50个clip，遵循与Sports相同的评估方案，但不保证Sports数据库与UCF-101没有重叠，但是我们只用每个video中的一些采样clip。使用slow fusion，从结果中看出，重新训练softmax（方案a）不是最好的，可能因为high-level的特征太集中于sports，而且fine-tune所有层也是不足够的（可能因为过拟合）
#### b、不同group的表现
&emsp;&emsp;将数据库分成5个组，计算每一类的平均精度，然后计算每个组的不同类的平均精度。从结果看，大部分良好的表现来源于UCF-101中的sports类别
![](/images/Large/group.png "不同group的结果")