---
title: 动作识别(Two-Stream Convolutional Networks for Action Recognition in Videos, NIPS 2014)
date: 2019-10-08 21:01:29
tags:
- 动作识别
- 双流网络
- 光流
- 多任务学习
---
&emsp;&emsp;本文的目的是扩展现有的用于单张图片分类的卷积网络的能力，用于视频数据中的动作识别，为了达到这个目标，探索了一种不同的方法，用两个独立的识别stream（空间，时间），并将两个stream合并得到最终的结果。空间stream用于从静止的视频帧中识别动作，时间stream用于从以密集光流为展现形式的运动特征中识别动作，两个stream都是卷积网络。两个网络独立设计也能充分利用在ImageNet上预训练的网络的能力。two-stream的设计是根据人类视觉皮层包含两条路径：腹侧流（进行物体识别）和背侧流（识别运动），这里我们不再继续讨论这种关联的设计。
# 一、双流网络
&emsp;&emsp;视频可以分成另个组成部分：时间+空间，空间部分，以单独的视频帧的形式存在，包含了场景和物体信息；时间部分，以帧之间的运动的形式存在，包含了观测者（相机）和目标的运动。双流网络的结构如下图所示，每个stream都是由卷积网络组成，将两个stream各自计算softmax分数后进行最终的融合。融合的方法包括两种：以堆叠的L2归一化softmax分数为特征进行平均化和训练一个多分类的线性SVM。
![](/images/two/two-stream.png "双流网络结构")
## 1、空间stream网络
&emsp;&emsp;有些动作是与特定的个体有强相关性的，从实验中也发现仅仅是单独的空间识别stream的效果已经是很不错的了。因为空间卷积网络在一个图片分类结构中是很必要的，而且可以在大的数据库中（如ImageNet）上进行预训练。
## 2、光流卷积网络
### 光流堆叠
&emsp;&emsp;密集光流可以看成是一些列是相邻帧的位移矢量场的集合，水平和竖直方向的矢量场可以看成是图像的通道维度，那么L个连续帧可以组成2L个输入的图像通道。对于任意一帧中的任意一个点定义为(u, v)，d<sub>t</sub>表示点(u. v)从第t帧到第t+1帧的位移变化。卷积网络的输入定义为：
<center>I<sub>t</sub>(u, v, 2k-1) = d<sup>x</sup><sub>t+k-1</sub>(u, v)</center>
<center>I<sub>t</sub>(u, v, 2k) = d<sup>y</sup><sub>t+k-1</sub>(u, v), u=[1;w], v=[1;h], k=[1;L]&emsp; (1)</center>
### 轨迹堆叠
&emsp;&emsp;另一种运动特征的表示，是基于轨迹的描述子，在视频帧中，在与光流法相同的位置，沿着运动轨迹采样，在这种方法中的输入定义为：
<center>I<sub>t</sub>(u, v, 2k-1) = d<sup>x</sup><sub>t+k-1</sub>(p<sub>k</sub>)</center>
<center>I<sub>t</sub>(u, v, 2k) = d<sup>y</sup><sub>t+k-1</sub>(p<sub>k</sub>), u=[1;w], v=[1;h], k=[1;L]&emsp;(2)</center>
&emsp;&emsp;其中，p<sub>k</sub>是沿轨迹采样的第k个点，在第t帧中开始于位置(u, v)，并有以下递推关系：
<center>p<sub>1</sub>=(u, v); p<sub>k</sub>=p<sub>k-1</sub> + d<sub>t+k-2</sub>(p<sub>k-1</sub>), k>1</center>
&emsp;&emsp;在光流法的公式中，通道I<sub>r</sub>(u, v, c)存储了位置(u, v)的位移向量，也就是说，光流堆叠采样了在视频帧中同一个位置的位移向量d；而基于轨迹的表示方法存储了在位置点p<sub>k</sub>，沿着轨迹采样的向量（如下图右侧所示）
![](/images/two/convnet_input.png "光流堆叠与轨迹堆叠")
### 双向光流
&emsp;&emsp;前面的光流法处理的是前向光流，也就是第t帧的位移场d<sub>t</sub>指定其像素在下一帧的位置，那么很自然的想到扩展成双向光流，即通过计算另一组相反方向的位移场来获得，之后可以通过堆叠第t帧和第t+L/2帧之间的L/2个前向流，和第t-L/2帧和第t帧之间的L/2个后向流，构建一个输入I<sub>t</sub>，因此输入I<sub>t</sub>就有和之前一样的通道数2L，这种方法可以用公式（1）和（2）中任意一个来表示。
### 减去平均流量法
&emsp;&emsp;通常对网络的输入使用zero-centering是有好处的，因为这使得模型更好地使用rectification非线性，在我们的例子中，位移矢量场元素既可以是正，也可以是负，并且很自然地集中在一点上：在各种各样的运动中，相同方向的运动出现的可能性与相反方向运动出现的可能性是一样的，但是给定相邻两个视频帧，他们之间的光流可以由特定的位移来控制，即由相机的运动造成。**相机运动补偿**的重要性在之前已经强调过，首先估计一个全局运动分量，然后从密集光流中减去即可。在我们的论文中，我们考虑一个更简单的方法：从每个位移场d中减去平均向量。
&emsp;&emsp;以上我们描述了几种在一个输入I<sub>t</sub>中组合不同光流位移场的方法，考虑到卷积网络需要一个固定尺寸的输入，我们从输入中采样一个224\*224\*2L大小的子输入作为输入，隐层的设置尽可能保持与空间网络一致。
## 3、多任务学习
&emsp;&emsp;空间stream网络可以在大的图片数据库，如ImageNet上进行预训练；时间stream需要在视频数据上训练，而可用的视频动作分类的数据库又很小，在我们的实验中，在UCF-101和HMDB-51数据库上训练，他们只有9.5K和3.7K个视频，为了减少过拟合，一个可用的方法是将两个数据库组合成一个，但是由于各个类别之间的交集使得这并不容易实现。可用的方法是只添加没有出现在原始数据库中的类别的图片，但是这个需要人工搜索这种类别并且限制了额外的训练数据的数量。
&emsp;&emsp;一个更合理的组合多个数据库的方法是基于多任务学习。其目标是学习一个视频表示，这不仅适用于所讨论的任务（例如HMDB-51分类），还适用于其他任务（例如UCF-101分类）。其他的任务扮演了正则化的角色，并且允许使用额外的训练数据。在我们的实验中，修改了卷积网络使它在最后的全连接层有**两个softmax分类器**：一个用于计算HMDB-51的分类分数；另一个用于计算UCF-101。每一层都有它自己的损失函数，整个训练过程中的loss由单个任务loss的加和组成，网络的权重导数可以通过反向传播计算得到。
## 4、实现细节
### 卷积网络的设置
&emsp;&emsp;空间和时间卷积网络的隐层均使用ReLU激活函数，max-pooling层的作用区域是3\*3，stride=2，空间网络与时间网络唯一的不同是，我们从时间网络中移除了第二个正则化层来减少内存消耗。
### 训练
&emsp;&emsp;优化器：mini-batch SGD，momentum=0.9，batch_size=256，每次迭代，从每个类别均匀采样256个训练视频，从每个视频中随机选择一帧。在空间网络的训练中，从选择的帧中crop出一个224\*224的子图片区域，通过随机的水平flip和RGB jittering，视频会事先进行缩放，使得最小的边的长度为256，而子图片区域是从整个帧中采样得到的，而不仅仅是256\*256的中心。在时间网络中，我们先为选择的训练视频帧计算一个光流输入I，再随机crop出一个224\*224\*2L的区域，并进行flip。lr=1e-2，并根据固定的时间减少，也就是说，若从头开始训练，经过5万轮迭代后lr变成1e-3，经过7万轮迭代后，lr=1e-4，经过8万轮迭代后训练停止；若是fine-tune，经过14k轮后lr=1e-3，经过2万轮后停止训练。
### 测试
&emsp;&emsp;在测试阶段，给定一段视频，我们先采样固定数量的帧（实验中是25），帧之间的时间步长保持一致，从每个视频帧中，通过clip四个角和中心，得到10个输入，整个视频的类别分数通过计算采样帧和变换后的帧的识别结果的平均值来得到。
### 在ImageNet ILSVRC-2012上进行预训练
&emsp;&emsp;当预训练空间网络时，我们使用与上述一样的扩增方法（flip，crop，RGB jittering）来扩增训练和测试数据，在数据库上的top-5错误率由16.0%降到了13.5%，我们相信，主要的原因是从整个图片进行采样，而不仅仅是图片中心。
### 多GPU训练
&emsp;&emsp;网络结构的实现是通过公开的Caffe toolbox，但是有一些比较重要的改变，包括装在单系统上的多GPU训练。我们使用数据并行策略，并将每个batch分成几份，在几个GPU上进行计算，训练一个单个的时间网络需要在用于4个NVIDIA Titan的单系统上训练一天，与单个GPU相比，速度节省了3.2倍。
### 光流
&emsp;&emsp;光流的计算是通过使用现成的OpenCV toolbox中的GPU应用来实现的，尽管计算速度很快（一对视频帧需要0.06s），但如果要实时计算，仍然存在瓶颈，所以在训练之前先计算光流。为了避免将位移场存储为float，水平和竖直两个分量都被缩放到了[0, 255]，并且使用JPEG压缩（解压缩后，光流重新缩放至原始范围），这使得UCF-101的光流大小由1.5TB减小到了27GB。
## 5、实验评估
### 数据库和评估标准
&emsp;&emsp;本文用到的两个数据库：UCF-101和HMDB-51。UCF-101包括13K个视频（平均每个视频180帧），标注成101类动作，训练数据包括9.5K个视频，；HMDB-51有6.8K个视频，包括51个动作，训练数据包括3.7K个视频。
### 空间卷积网络
&emsp;&emsp;首先评估空间stream网络的表现，考虑三个场景：从头训练UCF-101；在ILSVRC-2012上预训练，在UCF-101上fine-tune；固定预训练网络，只训练最后的分类层。dropout正则化比例为0.5或者0.9，结果如下所示。从结果中可以看出，仅在UCF-101上训练网络会导致过拟合（即使dropout的比例很大），而fine-tune整个网络只有微小的提升，而最后dropout比例高会导致过度正则化网络，导致错误的分类结果。在之后的实验中，我们使用只训练最后一层的策略。
![](/images/two/evaluation.png "空间网络和时间网络的识别结果")
### 时间卷积网络
&emsp;&emsp;评估时间卷积网络的策略：使用多个堆叠的光流（L={5, 10}）；堆叠轨迹；减去平均流量；使用双向光流。结构是从头训练UCF-101，dropout的比例为0.9，从结果上看，我们可以知道，堆叠多个（L>1）位移场是有好处的，因为给网络提供了长时的运动信息，比一对视频帧（L=1）中包含的流信息更有判别性。输入的流的数量从5增加到10，只得到了比较小的提升，所以在之后的实验中，我们保持L=10不变。其次，我们发现减去平均值是有效果的，因为降低了帧之间全局运动的影响，在之后的实验中作为默认操作使用。不同堆叠技术的区别是比较小的，而光流堆叠的方法比轨迹堆叠的方法好；使用双向光流的方法只略微优于使用单向的前向光流；时间网络的表现优于空间网络，证明运动信息在动作识别中的重要性。
&emsp;&emsp;除此之外，还使用了slow fusion策略，当从头训练UCF-101时，精度为56.4%，优于单帧结构52.3%，但依旧低于基于光流特征的网络，这说明尽管多帧信息很重要，但以合适的方式将其输入到卷积网络中也很重要。
### 时间卷积网络的多任务学习
&emsp;&emsp;在UCF-101上训练时间卷积网络是很有挑战性的，因为训练集比较小；而更大的挑战是在HMDB-51上训练，训练集更小，在实验中使用不同的方法来提升其训练的有效性：在UCF-101上预训练，在HDMB-51上fine-tune；通过人工选择，从UCF-101上增加78类，这些类别与原始的HMDB-51中的类别没有交叉；使用多任务来学习一个视频表示，在UCF-101和HMDB-51之间共享。结果如下所示，
![](/images/two/multi-task.png "多任务学习的识别结果")
### 双流卷积网络
&emsp;&emsp;将空间网络和时间网络组合起来，实验结果如下图所示，可以看出，时间网络和空间网络是互补的；基于SVM的softmax分数融合优于平均法；使用双向光流的表现不是很好；使用多任务学习来训练的时间卷积网络，在单独使用和与空间网络合并这两种情况中都是最好的。
![](/images/two/two-stream-eval.png "双流网络的识别结果")
### 与其他方法的比较
&emsp;&emsp;空间网络，在ILSVRC上预训练，在UCF或者HMDB上训练最后一层；时间网络，使用多任务学习训练UCF和HMDB；输入使用非双向光流堆叠，减去平均值；两个网络的softmax分数通过平均法或者SVM和平。通过结果可以看到，单独的空间网络和时间网络都远远优于深度网络结构，两个网络的的组合更是提升了结果。
![](/images/two/all.png "与其他方法的对比")
&emsp;&emsp;
&emsp;&emsp;


