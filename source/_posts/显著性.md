---
title: 显著性
date: 2018-04-07 16:54:55
tags:
- 显著性
- CNN
- 多级输出
---
&emsp;&emsp;在长期的进化中，人类拥有了在复杂环境中快速发现感兴趣目标的能力，这种高度发达的注意力机制使人们在看一张图片时，其注意点首先会落在更能刺激视觉的区域，即显著性区域。目前我们拥有海量的图像数据，而显著性目标检测即模仿人类的视觉注意力机制，在图片数据中找到最重要的信息，进行后续处理。  
&emsp;&emsp;显著性目标检测的应用有：自适应压缩，内容感知的图像编辑，图像检索，缩略图裁剪，图像增强等。实际应用中，Twitter推出了图像的[智能裁切功能](https://blog.twitter.com/engineering/en_us/topics/infrastructure/2018/Smart-Auto-Cropping-of-Images.html)，在预览时，能自动显示显著性区域，效果如下图所示，
  ![](/images/智能裁切.png "Twitter智能裁切")
&emsp;&emsp;这里要介绍的是南开大学程明明教授的DSS方法，该方法目前已应用在华为mate10上，[大光圈智能拍照](http://news.nankai.edu.cn/nkyw/system/2017/12/24/000362595.shtml)。在介绍DSS之前，首先要引入两个参考框架：FCN和HED。
一、FCN
&emsp;&emsp;FCN(Fully Convolutional Networks)，将图像级别的分类扩展成像素级的分类，实现了语义分割，首先将VGG最后的三个全连接层换成了卷积层，然后分别独立输出三个尺度的分割结果。（在最后一层反卷积，或转置卷积进行dense prediction）
![](/images/fcn_vgg.png "FCN网络")
&emsp;&emsp;文章中的图及各个尺度的分割结果：
![](/images/FCN.png "FCN结构") 
![](/images/分割结果.png "分割结果")
&emsp;&emsp;FCN的8s,16s和32s对应三种尺度的输出，但都是独立的分割结果，彼此之间没有联系。
&emsp;&emsp;[caffe源代码](https://github.com/shelhamer/fcn.berkeleyvision.org)，[keras版本](https://github.com/aurora95/Keras-FCN)，[tensorflow版本](https://github.com/MarvinTeichmann/tensorflow-fcn)

二、HED（Holistically-Nested Edge Detection）
1、HED的特点
&emsp;&emsp;HED是用于边缘检测的网络，思路基于FCN。以前的基于CNN的边缘检测网络多睡patch-to-class模式，使用滑窗的的方法，以patch为中心。在FCN出现后，直接应用FCN做边缘检测效果不佳，原因可能是边缘检测不同于语义分割，需要更多的多尺度信息来找到边缘。所以，基于FCN，但又不同于FCN，HED结构的特点：
1）类似FCN，是image-to-image的
2）加入了deep supervision，学习不同尺度的特征
3）整合side output，即不同尺度的预测结果。
2、网络结构
&emsp;&emsp;HED的网络结构如下图所示，在VGG的基础上，首先去掉了最后的全连接层，减少计算和存储；其次，考虑到stride=32时，生成的预测图太模糊，边缘已经不明显，所以去掉了最后一个pooling层；最后，以pooling为界，对卷积操作分组，在每一组的最后一个卷积层之后，经过1x1卷积和反卷积操作，生成与原图相同大小的feature map，然后经过sigmoid激活函数，得到每一组最终的预测mask（文章中的side output）。最后，将五组预测mask以concat的方式组合，经过1x1卷积和sigmoid，得到融合的预测mask（fused mask）。
![](/images/hed_vgg.png "HED网络结构")
&emsp;&emsp;在预测时，最终的mask是五个side output和fused mask的平均值。 
![](/images/final_predict.png "预测时最终mask的计算方法")
3、损失函数
&emsp;&emsp;HED的损失函数分别两个部分：Deep supervision和Weighted-fusion supervision。Deep supervision是对每个side output的监督，定义为：
![](/images/deep_supervision.png "Deep supervision")
其中，*lside*是第m个side output的loss，定义为交叉熵，其中*beta*是为了平衡正负样本，因为在边缘检测的ground truth中，大部分像素都是非边缘，只有极少部分是边缘，所以正负样本不均衡，如果用一般的交叉熵，loss会有偏重，这里为了减缓这种现象，加入了beta，beta定义为非边缘像素点的个数占总像素点个数的比例。alpha是第m个side output的loss在总的Lside中占的比例，论文中设置为1，每个side output对总的loss的贡献是一样的。
&emsp;&emsp;第二部分是weighted-fusion supervision，定义为fused mask对应的损失函数，计算方法如下：
![](/images/weighted_loss.png "Weighted-fusion supervision")
其中，h是在得到fused mask时，每个side output所占比例，是由网络自己去学习的，*Lfuse*定义为fused mask与ground truth的距离，实际上也是交叉熵。最终的损失函数是deep supervision与weighted-fusion supervision相加。
4、实验
&emsp;&emsp;在实验部分，作者对比了FCN-2s,8s以及有无deep supervision的结果，并可视化了有无deep supervision的结果：
![](/images/exper_table.png "方法的对比")
![](/images/exper_vis.png "可视化结果")
结果显示，有deep supervision时，网络的side output趋向于自然直观，得到的预测图是由粗到细，由局部到整体的结构；否则，网络学到的是无序的，且偏重于学习大结构的边缘。

三、DSS
