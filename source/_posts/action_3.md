---
title: 动作识别(Two-Stream Convolutional Networks for Action Recognition in Videos, NIPS 2014)
date: 2019-10-08 21:01:29
tags:
- 动作识别
- 双流网络
- 光流
- 多任务学习
---
&emsp;&emsp;相对于单张图片的分类，视频相当于提供了一个自然的数据扩增方案。
&emsp;&emsp;本文的目的是扩展现有的用于单张图片分类的卷积网络的能力，用于视频数据中的动作识别，为了达到这个目标，探索了一种不同的方法，用两个独立的识别stream（空间，时间），并将两个stream在最后合并得到最终的结果。空间stream用于从静止的视频帧中识别动作，时间stream用于从以密集光流为展现形式的运动特征中识别动作，两个stream都是卷积网络。两个网络独立设计也能充分利用在ImageNet上预训练的网络的能力。two-stream的设计是根据人类视觉皮层包含两条路径：腹侧流（进行物体识别）和背侧流（识别运动），这里我们不再继续讨论这种关联的设计。
# 一、双流网络
&emsp;&emsp;视频可以分成另个组成部分：时间+空间，空间部分，以单独的视频帧的形式存在，包含了场景和物体信息；时间部分，以帧之间的运动的形式存在，包含了观测者（相机）和目标的运动。双流网络的结构如下图所示，每个stream都是由卷积网络组成，将两个stream各自计算softmax分数后进行最终的融合。融合的方法包括两种：以堆叠的L2归一化softmax分数为特征进行平均化和训练一个多分类的线性SVM。
![](/images/two/two-stream.png "2D卷积和3D卷积的对比")
## 1、空间stream网络
&emsp;&emsp;有些动作是与特定的个体有强相关性的，从实验中也发现仅仅是单独的空间识别stream的效果已经是很不错的了。因为空间卷积网络在一个图片分类结构中是很必要的，而且可以在大的数据库中（如ImageNet）上进行预训练。
## 2、光流卷积网络
### 光流的叠
&emsp;&emsp;密集光流可以看成是一些列是相邻帧的位移矢量场的集合，水平和竖直方向的矢量场可以看成是图像的通道维度，那么L个连续帧可以组成2L个输入的图像通道。对于任意一帧中的任意一个点定义为(u, v)，d<sub>t</sub>表示点(u. v)从第t帧到第t+1帧的位移变化。卷积网络的输入定义为：
I<sub>t</sub>(u, v, 2k-1) = d<sup>x</sup><sub>t+k-1</sub>(u, v)
I<sub>t</sub>(u, v, 2k) = d<sup>y</sup><sub>t+k-1</sub>(u, v), u=[1;w], v=[1;h], k=[1;L]   (1)
### 轨迹堆叠
&emsp;&emsp;另一种运动特征的表示，是基于轨迹的描述子，在视频帧中，在与光流法相同的位置，沿着运动轨迹采样，在这种方法中的输入定义为：
I<sub>t</sub>(u, v, 2k-1) = d<sup>x</sup><sub>t+k-1</sub>(p<sub>k</sub>)
I<sub>t</sub>(u, v, 2k) = d<sup>y</sup><sub>t+k-1</sub>(p<sub>k</sub>), u=[1;w], v=[1;h], k=[1;L]   (2)
&emsp;&emsp;其中，p<sub>k</sub>是沿轨迹采样的第k个点，在第t帧中开始于位置(u, v)，并有以下递推关系：
p<sub>1</sub>=(u, v); p<sub>k</sub>=p<sub>k-1</sub> + d<sub>t+k-2</sub>(p<sub>k-1</sub>), k>1
&emsp;&emsp;在光流法的公式中，通道I<sub>r</sub>(u, v, c)存储了位置(u, v)的位移向量，也就是说，光流堆叠采样了在视频帧中同一个位置的位移向量d；而基于轨迹的表示方法存储了在位置点p<sub>k</sub>，沿着轨迹采样的向量（如下图右侧所示）
![](/images/two/convnet_input.png "光流堆叠与轨迹堆叠")
### 双向光流
&emsp;&emsp;前面的光流法处理的是前向光流，也就是第t帧的位移场d<sub>t</sub>指定其像素在下一帧的位置，那么很自然的想到扩展成双向光流，即通过计算另一组相反方向的位移场来获得，之后可以通过堆叠第t帧和第t+L/2帧之间的L/2个前向流，和第t-L/2帧和第t帧之间的L/2个后向流，构建一个输入I<sub>t</sub>，因此输入I<sub>t</sub>就有和之前一样的通道数2L，这种方法可以用公式（1）和（2）中任意一个来表示。
### 减去平均流量法
&emsp;&emsp;通常对网络的输入使用zero-centering是有好处的，因为这是的模型更好地使用rectification非线性，在我们的例子中，位移矢量场元素既可以是正，也可以是负，并且很自然地集中在一点上：在各种各样的运动中，一个方向运动出现的可能性与相反方向运动的可能性是一样的，但是给定相邻两个视频帧，他们之间的光流可以由特定的位移来控制，即由相机的运动造成。相机运动补偿的重要性在之前已经强调过，首先估计一个全局运动分量，然后从密集光流中减去即可。在我们的论文中，我们考虑一个更简单的方法：从每个位移场d中减去平均向量。
&emsp;&emsp;以上我们描述了几种在一个输入I<sub>t</sub>中组合不同光流位移场的方法，考虑到卷积网络需要一个固定尺寸的输入，我们从输入中采样一个224\*224\*2L大小的子输入作为输入，隐层的设置尽可能保持与空间网络一致。
## 3、多任务学习
&emsp;&emsp;空间stream网络可以在大的图片数据库，如ImageNet上进行预训练；时间stream需要在视频数据上训练，而可用的视频动作分类的数据库又很小，在我们的实验中，在UCF-101和HMDB-51数据库上训练，他们只有9.5K和3.7K个视频，为了减少过拟合，一颗可以思考力的方法是将两个数据库组合成一个，但是由于各个类别之间的交集使得这并不容易实现。一个选择是只添加没有出现在原始数据库中的类别的图片，但是这个需要人工搜索这种类别并且限制了额外的训练数据的数量。
&emsp;&emsp;一个更合理的组合多个数据库的方法是基于多任务学习。其目标是学习一个视频表示，这不仅适用于所讨论的任务（例如HMDB-51分类），还适用于其他任务（例如UCF-101分类）。其他的任务扮演了正则化的角色，并且允许使用额外的训练数据。在我们的实验中，修改了卷积网络使它在最后的全连接层有**两个softmax分类器**：一个用于计算HMDB-51的分类分数；另一个用于计算UCF-101.每一层都有它自己的损失函数，整个训练过程中的loss由单个任务loss的加和组成，网络的权重导数可以通过反向传播计算得到。
## 4、实现细节
### 卷积网络的设置
&emsp;&emsp;空间和时间卷积网络的隐层均使用ReLU激活函数，max-pooling层的作用区域是3\*3，stride=2，空间网络与时间网络唯一的不同是，我们从时间网络中移除了第二个正则化层来减少内存消耗。
### 训练
&emsp;&emsp;优化器：mini-batch SGD，momentum=0.9，batch_size=256，每次迭代，从每个类别均匀采样256个训练视频，从每个视频中随机选择一帧。在空间网络的训练中，从选择的帧中crop出一个224\*224的子图片区域，通过随机的水平flip和RGB jittering，视频会事先进行缩放，使得最小的边的长度为256，而子图片区域是从整个帧中采样得到的，而不仅仅是256\*256的中心。在时间网络中，我们先为选择的训练视频帧计算一个光流输入I，再随机crop出一个224\*224\*2L的区域，并进行flip。lr=1e-2，并根据固定的时间减少，也就是说，若从头开始训练，经过5万轮迭代后lr变成1e-3，经过7万轮迭代后，lr=1e-4，经过8万轮迭代后训练停止；若是fine-tune，经过14k轮后lr=1e-3，经过2万轮后停止训练。
### 测试
&emsp;&emsp;在测试阶段，给定一段视频，我们先采样固定数量的帧（实验中是25），帧之间的时间步长保持一致，从每个视频帧中，通过clip四个角和中心，得到10个输入，整个视频的类别分数通过计算采样帧和变换后的帧的识别结果的平均值来得到。
### 在ImageNet ILSVRC-2012上进行预训练
&emsp;&emsp;当预训练空间网络时，我们使用与上述一样的扩增方法（flip，crop，RGB jittering）来扩增训练和测试数据，在数据库上的top-5错误率有16.0%降到了13.5%，我们相信，主要的原因是从整个图片进行采样，而不仅仅是图片中心。
### 多GPU训练
&emsp;&emsp;网络结构的实现是通过公开的Caffe toolbox，但是有一些比较重要的改变，包括装在单系统上的多GPU训练。我们使用数据并行策略，并将每个batch分成几份，在几个GPU上进行计算，训练一个单个的时间网络需要在用于4个NVIDIA Titan的单系统上训练一天，与单个GPU相比，速度节省了3.2倍。
### 光流
&emsp;&emsp;光流的计算是通过使用现成的OpenCV toolbox中的GPU应用来实现的，尽管计算速度很快（一对视频帧需要0.06s），但如果要实时计算，仍然存在瓶颈，所以在训练之前先计算光流。为了避免将位移场存储为float，水平和竖直两个分量都被缩放到了[0, 255]，并且使用JPEG压缩（解压缩后，光流重新缩放至原始范围），这使得UCF-101的光流大小由1.5TB减小到了27GB。
&emsp;&emsp;

